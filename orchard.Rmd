---
title: "Make a world - Apples"
author: "Daniel Ricard and breakout group participants"
date: '`r paste0("Last modified timestap: ", format(Sys.time(), "%Y-%m-%d %H:%M:%S"))`'
output: html_notebook
---
```{r, echo=FALSE}
library(ggplot2)
```

As part of a [TESA webinar](https://www.gcpedia.gc.ca/wiki/DFO_Science_TESA_ETES#Outputs_and_archive_of_material_from_some_previous_activities) that we did in October 2018, I had used a recent visit to an apple orchard to introduce the concepts of catchability and selectivity in fisheries surveys.

# Deterministic case
Let's look at the deterministic case:

## Uniform orchard

```{r deterministic-uniform}
nx <- 100
ny <- 100
n.trees.uni <- nx * ny
n.apples.per.tree.uni <- 100
n.apples.uni <- n.trees.uni * n.apples.per.tree.uni
```
which is just saying that there are `r format(n.trees.uni, big.mark=",")` trees in the orchard, with a total of `r format(n.apples.uni, big.mark=",")` apples.

This is completely unnecessary here, but will become useful later, let's create a data frame with all the information about the orchard.
```{r uniform1}
uniform.orchard.df <- data.frame(tree.number=1:n.trees.uni, n.apples=n.apples.per.tree.uni)
```
Not surprisingly, the simulated data frame informs us that there are still `r format(nrow(uniform.orchard.df), big.mark=",")` trees in the orchard, with a total of `r format(sum(uniform.orchard.df$n.apples), big.mark=",")` apples.

To estimate the number of apples in the uniform orchard, we only need to sample one tree, and we learn nothing from adding additional samples . We will never overestimate the number of apples in the orchard in this scenario because we have perfect knowledge of the system. Increasing the number of samples does not change our perception of the system.

```{r uniform2}
sampling.df <- data.frame(n.samples=1:100,estimate=NA)
for(i in 1:nrow(sampling.df)){
  sampling.df[i,"estimate"] <- mean(uniform.orchard.df[sample(seq(0, n.trees.uni), sampling.df[i,"n.samples"],replace=FALSE), "n.apples"]) * n.trees.uni
}
g <- ggplot(sampling.df, aes(x=n.samples, y=estimate)) +
  geom_line()
g
```

This figure above conveys the information correctly, but it is erroneous in the way it examines the number of samples. Let's fix it by instead selecting 100 tree numbers at random, and then looking at the estimate we obtain as each additional tree is added.

## Two-tier orchard
Let's now simulate an orchard where 50%$ of the trees have 100 apples and the other 50% of the tress have 50 apples.

```{r deterministic-twotier}
nx <- 100
ny <- 100
prop.bad <- 0.5
n.trees <- nx * ny
n.apples.per.good.tree <- 100
n.apples.per.bad.tree <- 50
n.apples <- (n.trees*(1-prop.bad) * n.apples.per.good.tree) + (n.trees*prop.bad * n.apples.per.bad.tree)
```
which is just saying that there are `r format(n.trees, big.mark=",")` trees in the orchard, with a total of `r format(n.apples, big.mark=",")` apples.


```{r twotier1}
twotier.orchard.df <- data.frame(tree.number=1:n.trees, n.apples=rep(c(n.apples.per.good.tree, n.apples.per.bad.tree), n.trees/2))
```
Not surprisingly, the simulated data frame informs us that there are still `r format(nrow(twotier.orchard.df), big.mark=",")` trees in the orchard, with a total of `r format(sum(twotier.orchard.df$n.apples), big.mark=",")` apples.

To correctly estimate the number of trees in the two-tier orchard, we need to sample as little as two trees, if by chance we measure one good tree and one bad tree. As we add additional samples, and we get closer to a 50-50 split between good and bad trees, our estimate will converge towards the true number of apples. We will overestimate the number of apples in the orchard when we have sampled more good trees than bad trees.

For looking at this, let's simulate how our estimate changes as we increase our number of samples. To investigate how additional samples improve our estimate, we will simulate sampling 100 trees and keeping track of our running estimate as we go. 
```{r twotier2}
tree.numbers <- sample(seq(n.trees), 100)
sampling.df <- data.frame(n.samples=1:100,estimate=NA)
for(i in 1:100){
  sampling.df[i, "estimate"] <- mean(twotier.orchard.df[tree.numbers[1:i], "n.apples"]) * n.trees
}
g <- ggplot(sampling.df, aes(x=n.samples, y=estimate)) +
  geom_line() + ylim(5E05, 1E06) + geom_abline(intercept=sum(twotier.orchard.df$n.apples))
g
```
Note how the graph above is only one realisation of obtaining 100 samples, and it will change each time the code above is run because of the "sample" function. 


```{r twotier3}

## plot a large number of realisations of obtaining 100 samples
n.sim <- 1000

n.samp <- 100
est.mat <- matrix(NA, nrow = n.sim, ncol = n.samp)
dimnames(est.mat) <- list(sim = seq(n.sim), n.samp = seq(n.samp))
for(i in seq(n.sim)){
  for(j in seq(n.samp)){
    sample.trees <- sample(seq(n.trees), j)
    est.mat[i, j] <- mean(twotier.orchard.df[sample.trees, "n.apples"]) * n.trees
  }

plot(1:100, rep(1E06,100), type='n', ylim=c(5E05,1E06))
for(n in 1:n.sim){
  
tree.numbers <- sample(seq(0, n.trees), 100)
sampling.df <- data.frame(n.samples=1:100,estimate=NA)
for(i in 1:100){
  sampling.df[i, "estimate"] <- mean(twotier.orchard.df[tree.numbers[1:i], "n.apples"]) * n.trees
  
}
lines(1:100, sampling.df$estimate, colour=adjustcolor('black', alpha=05))
deterministic.list[[n]] <- sampling.df
}
abline(a=sum(twotier.orchard.df$n.apples), b=0, lty=1)

## what is the probability of overestimating the true number of apples?
## when n=1 samples,
for (nn in 1:20){
nnn <- do.call(rbind, lapply(1:n.sim, function(i){deterministic.list[[i]][nn,"estimate"]}))
tt <- table(nnn > sum(twotier.orchard.df$n.apples))
print(paste0("n=", nn, " p(overestimate) = ", tt[2]/sum(tt)))

}
est.df <- as.data.frame.table(est.mat, responseName = "est", stringsAsFactors = FALSE)
# plot(est ~ n.samp, data = est.df)
colMeans(est.mat)
colMeans(est.mat > sum(twotier.orchard.df$n.apples))
colMeans(est.mat == sum(twotier.orchard.df$n.apples))

plot(colMeans(est.mat) - sum(twotier.orchard.df$n.apples))

```

Why is this the case that our number of samples doesn't improve our chance of not overestimating the number of apples in the orchard? And why is it not 50% across the board? Why are the probabilities of overestimating the number of apples smaller when we have an odd number of samples? There is a deterministic explanation to all of the above since there is no stochasticity involved.

*Paul Regular comment:* This is a great example of how a simulation can create unexpected and curious patterns. After some digging, I think the deterministic explanation behind the odd/even pattern is that there is a chance of receiving the exact answer when there are an even number of samples. For instance, when only 2 trees are sampled, there is a 50% chance that the estimate will be exact (i.e. neither over or under estimated).


The performance measure that we have identified is the probability of overestimating the number of apples in the orchard, which we can calculate using our simulations.

# Stochastic case
```{r stochastic-uniform}
nx <- 100
ny <- 100
n.trees.sto <- nx * ny
n.apples.sto <- sum(rpois(n.trees.sto, 100))
```
A total of `r format(n.apples.sto, big.mark=",")` apples in the orchard. Note how each time the code chunk above is run, the number of apples in the orchard will change.

What happens if we simulate 1000 different orchards, what is the estimate of the number of apples in the orchard?
```{r}
hist(1000*rpois(n.trees.sto, 100))
```
This should be no surprise since the [sum of Poisson distributions is also a Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution#Sums_of_Poisson-distributed_random_variables). So the resulting answer is a Poisson distribution with a mean of 1E06. But what about the variance around the mean?

```{r}
hist(rpois(1000, 1E06))
```


## Comparing two sampling strategies
Sampling all the apples in a tree is a pretty involved process, and to save time we may want to sample just the apple that we can reach from the ground instead. 

If we can assume that the apples are homogeneously distributed in a tree, and that we know the proportion of the tree sampled by an observer (height of observer divided by height of the tree), we would not loose any information by sampling this way. So let's test this statement.

First we will create 1000 different realisations of the orchard.


```{r stochastic1}
n.trees.sto <- 10000

n.sims <- 1000

sto.df <- do.call(
  rbind,
  lapply(1:n.sims, 
         function(s){data.frame(sim.number=s, tree.number=1:n.trees.sto, n.apples=rpois(n.trees.sto, 100))}
  )
  
)


summary(sto.df)
```
Let's now compare our two sampling strategies.

```{r strategy1}
stochastic.list <- list()

## plot a large number of realisations of obtaining 30 samples
n.sims <- 1000

plot(1:30, rep(1E06,30), type='n', ylim=c(5E05,1.5E06))
for(n in 1:n.sims){
  
  tree.numbers <- sample(seq(0, n.trees), 30)
  sampling.df <- data.frame(n.samples=1:30,estimate=NA)
  for(i in 1:30){
    sampling.df[i, "estimate"] <- mean(sto.df[tree.numbers[1:i], "n.apples"]) * n.trees.sto
  }
  lines(1:30, sampling.df$estimate)
  stochastic.list[[n]] <- sampling.df
}
abline(a=sum(sto.df$n.apples), b=0, lty=1)

## 
## what is the probability of estimating the true number of apples with more than 10% error?
for(i in 1:10){
  tt <- table(do.call(rbind, lapply(1:n.sims, function(ns){abs(stochastic.list[[ns]][i,"estimate"] - sum(sto.df[sto.df$sim.number==ns,"n.apples"])) > 0.10*sum(sto.df[sto.df$sim.number==ns,"n.apples"])})))
  print(paste0("n=", i, " p(estimate off by more than 10%)=", tt[2]/sum(tt)))
}

```

Let's define a performance measure that encapsulates both the amount of time required to obtain samples and the precision and bias of the samples that we obtain. Let's assume that it takes 10 times longer to sample all the apples in a tree instead of just picking apples from a standing position.



How computationally expensive is it to run the same simulation use a loop compared to using lapply?

```{r}
n.sims <- 10000

## use a loop
loop.list <-list()
start_time <- Sys.time()
for(n in 1:n.sims){
  loop.list[[n]] <- data.frame(sim.num=n, rpois(10000, 10))
}
end_time <- Sys.time()

print(paste0("simulating 10000 times 10000 poisson observations in a loop took ", end_time - start_time, " seconds"))


## use lapply
start_time <- Sys.time()
lapply.list <-lapply(1:n.sims, function(n){data.frame(sim.num=n, rpois(10000, 10))})
end_time <- Sys.time()

print(paste0("simulating 10000 times 10000 poisson observations using lapply took ", end_time - start_time, " seconds"))

```

This actually surprises me, I thought that using sapply would speed things up.

